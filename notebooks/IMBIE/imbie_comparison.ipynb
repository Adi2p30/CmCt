{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ice sheet model should be a netCDF file. \n",
    "\n",
    "\n",
    "### `Lithk` variable\n",
    "The uploaded model to contain thickness data (the `lithk` variable) for the comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "import cftime \n",
    "import datetime\n",
    "from datetime import timedelta \n",
    "\n",
    "\n",
    "# note: suppress numpy.dtype size changed warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure IMBIE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the flag for the ice sheet region Greenland or Antarctica\n",
    "icesheet = \"GIS\"# Change to \"AIS\" or \"GIS\"\n",
    "\n",
    "# Set start and end dates\n",
    "start_date = '2006-01-01'\n",
    "end_date ='2014-01-01'\n",
    "\n",
    "#Set density of ice\n",
    "rho_ice = 918 # (kg/m^3)\n",
    "\n",
    "#output file dirctory\n",
    "output_path='/home/jovyan/CmCt/notebooks/IMBIE/'\n",
    "\n",
    "# Set model model path,shapefile path and projection and IMBIE csv_file\n",
    "if icesheet == \"GIS\":\n",
    "    projection = \"EPSG:3413\"  # Greenland\n",
    "    #Set the model data dir path\n",
    "    # nc_filename='/home/jovyan/shared-public/CmCt/models/ISMIP6/lithk_GIS_JPL_ISSM_asmb.nc'\n",
    "    nc_filename='/home/jovyan/shared-public/CmCt/models/ISMIP6/lithk_GIS_ILTS_PIK_SICOPOLIS1_historical.nc'\n",
    "    #Set the shape data dir path\n",
    "    shape_filename = \"/home/jovyan/CmCt/data/IMBIE/Greenland_Basins_PS_v1.4.2/Greenland_Basins_PS_v1.4.2.shp\"\n",
    "    #Set the observation data dir path\n",
    "    # obs_filename = '/home/jovyan/CmCt/data/IMBIE/imbie_greenland_2022_Gt_partitioned_v0.csv'\n",
    "    obs_filename = '/home/jovyan/CmCt/data/IMBIE/imbie_greenland_2021_Gt.csv'\n",
    "    \n",
    "elif icesheet== \"AIS\":\n",
    "    projection = \"EPSG:3031\"  # Antarctica    \n",
    "    #Set the model data dir path\n",
    "    nc_filename='/home/jovyan/CmCt/notebooks/Gravimetry/lithk_AIS_AWI_PISM1_hist_std.nc'\n",
    "    #Set the shape data dir path\n",
    "    shape_filename = \"/home/jovyan/CmCt/data/IMBIE/ANT_Basins_IMBIE2_v1.6/ANT_Basins_IMBIE2_v1.6.shp\"\n",
    "    #Set the observation data dir path\n",
    "    obs_filename = '/home/jovyan/CmCt/data/IMBIE/imbie_antarctica_2022_Gt_partitioned_v0.csv'\n",
    "    # obs_filename = '/home/jovyan/CmCt/notebooks/IMBIE/imbie_antarctica_2021_Gt.csv'\n",
    "    \n",
    "    ##Set the Region observation data dir path\n",
    "    obs_east_filename = '/home/jovyan/CmCt/data/IMBIE/imbie_east_antarctica_2022_Gt_partitioned_v0.csv'\n",
    "    obs_west_filename = '/home/jovyan/CmCt/data/IMBIE/imbie_west_antarctica_2022_Gt_partitioned_v0.csv'\n",
    "    obs_peninsula_filename= '/home/jovyan/CmCt/data/IMBIE/imbie_antarctic_peninsula_2022_Gt_partitioned_v0.csv'\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid iceshee value. Must be 'Greenland' or 'Antarctica'.\")\n",
    "\n",
    "\n",
    "# Select  variable for mass balance comparision\n",
    "mass_balance_column=\"Cumulative mass balance (Gt)\"# \"Cumulative dynamics mass balance anomaly (Gt)\"\n",
    "if mass_balance_column == \"Cumulative mass balance (Gt)\":\n",
    "    mass_balance_type = \"total\"\n",
    "elif mass_balance_column == \"Cumulative dynamics mass balance anomaly (Gt)\":\n",
    "    mass_balance_type = \"dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if  observation file exists\n",
    "if not os.path.exists(obs_filename):\n",
    "    raise FileNotFoundError(f\"Observation file not found: {obs_filename}\")\n",
    "\n",
    "# Check if model file exists    \n",
    "if not os.path.exists(nc_filename):\n",
    "    raise FileNotFoundError(f\"Model file not found: {nc_filename}\")\n",
    "\n",
    "\n",
    "if icesheet== \"AIS\":   \n",
    "    # Check if regional observation files exist \n",
    "    if not os.path.exists(obs_east_filename):\n",
    "        raise FileNotFoundError(f\"Observation file not found: {obs_east_filename}\")\n",
    "    if not os.path.exists(obs_west_filename):\n",
    "        raise FileNotFoundError(f\"Observation file not found: {obs_west_filename}\")\n",
    "    if not os.path.exists(obs_peninsula_filename):\n",
    "        raise FileNotFoundError(f\"Observation file not found: {obs_peninsula_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model data\n",
    "\n",
    "nc_filename='/home/jovyan/shared-public/CmCt/models/ISMIP6/lithk_GIS_BGC_BISICLES_historical.nc'\n",
    "gis_ds = xr.open_dataset(nc_filename)\n",
    "lithk = gis_ds['lithk']\n",
    "time_var = gis_ds['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the selcted dates are within the range of model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the start and end date according to the time variable of model data\n",
    "def adjust_for_calendar(calendar_type, date_dt, time_var):\n",
    "    \"\"\"\n",
    "    Adjusts the date format to match the type of the time variable in the dataset.\n",
    "    If the calendar is '360_day', adjust the day component accordingly.\n",
    "    \"\"\"\n",
    "    # If the time_var is in numpy.datetime64, return the date as np.datetime64\n",
    "    if isinstance(time_var.values[0], np.datetime64):\n",
    "        return np.datetime64(date_dt)\n",
    "\n",
    "    # If the time_var uses a cftime calendar, handle accordingly\n",
    "    elif isinstance(time_var.values[0], cftime.datetime):\n",
    "        if calendar_type == '360_day' and date_dt.day > 30:\n",
    "            # In a 360-day calendar, each month has only 30 days.\n",
    "            return cftime.datetime(date_dt.year, date_dt.month, 30, calendar=calendar_type)\n",
    "        else:\n",
    "            # For other calendars, use the original date.\n",
    "            return cftime.datetime(date_dt.year, date_dt.month, date_dt.day, calendar=calendar_type)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported time format in the dataset.\")\n",
    "        \n",
    "\n",
    "def convert_to_model_calendar(time_var, start_date, end_date):\n",
    "    # Parse input dates to datetime objects\n",
    "    start_date_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Extract the calendar type used by the time variable\n",
    "    try:\n",
    "        calendar_type = time_var.to_index().calendar\n",
    "        # print(calendar_type)\n",
    "    except AttributeError:\n",
    "        # Default to the 'standard' or 'gregorian' calendar if calendar attribute doesn't exist\n",
    "        calendar_type = 'standard'\n",
    "    \n",
    "    # Convert the start_date and end_date to the correct calendar type\n",
    "    start_date_cftime = adjust_for_calendar(calendar_type, start_date_dt,time_var)\n",
    "    end_date_cftime = adjust_for_calendar(calendar_type, end_date_dt,time_var)\n",
    "    \n",
    "    # print(f\"Start date in {calendar_type} calendar: {start_date_cftime}\")\n",
    "    # print(f\"End date in {calendar_type} calendar: {end_date_cftime}\")\n",
    "    \n",
    "    return start_date_cftime, end_date_cftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected dates 2006-01-01 and 2014-01-01 are within the range of the model data.\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum values directly from the time variable\n",
    "min_time = time_var.values.min()\n",
    "max_time = time_var.values.max()\n",
    "\n",
    "fomatted_start_date, fomatted_end_date =convert_to_model_calendar(time_var, start_date, end_date)\n",
    "\n",
    "# Check if the selected start and end dates are within the range\n",
    "if min_time <= fomatted_start_date <= max_time and min_time <= fomatted_end_date <= max_time:\n",
    "    print(f\"The selected dates {start_date} and {end_date} are within the range of the model data.\")\n",
    "else:\n",
    "    raise ValueError(f\"Error: The selected dates {start_date} or {end_date} are out of range. Model data time range is from {min_time} to {max_time}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate  model mass balance for each basin and total mass balance for whole region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate lithk values at the start and end dates\n",
    "lithk_start = lithk.interp(time=start_date).data.transpose().flatten()\n",
    "lithk_end = lithk.interp(time=end_date).data.transpose().flatten()\n",
    "\n",
    "# Calculate the difference\n",
    "lithk_delta = lithk_end - lithk_start\n",
    "\n",
    "# Replace NaN values with 0\n",
    "lithk_delta[np.isnan(lithk_delta)] = 0\n",
    "\n",
    "\n",
    "# Change Ice thickness unit from (m) to mass (kg) to gigatonnes(Gt)\n",
    "# ice thickness*area* density of ice* 1e-12\n",
    "#calculate area = x_resolution*y_resolution\n",
    "x_coords = gis_ds['x'].values\n",
    "y_coords = gis_ds['y'].values\n",
    "x_resolution = abs(x_coords[1] - x_coords[0])\n",
    "y_resolution = abs(y_coords[1] - y_coords[0])\n",
    "\n",
    "lithk_delta = (lithk_delta * x_resolution*y_resolution)*rho_ice * 1e-12\n",
    "\n",
    "\n",
    "# Create a list of Point geometries from coordinate grids\n",
    "points = [Point(x, y) for x in x_coords for y in y_coords]\n",
    "\n",
    "# Flatten lithk_delta to match the points list \n",
    "lithk_delta_flat = lithk_delta.flatten()\n",
    "\n",
    "# Create DataFrame\n",
    "lithk_df = pd.DataFrame({\n",
    "    'geometry': points,\n",
    "    'lithk_delta': lithk_delta_flat\n",
    "})\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "lithk_gdf = gpd.GeoDataFrame(lithk_df, geometry='geometry', crs=projection)\n",
    "\n",
    "# Load basin shapefile \n",
    "basins_gdf = gpd.read_file(shape_filename)\n",
    "\n",
    "# Perform spatial join\n",
    "joined_gdf = gpd.sjoin(lithk_gdf, basins_gdf, how=\"inner\", predicate='intersects')\n",
    "\n",
    "# Sum lithk_delta values by basin\n",
    "if icesheet == \"GIS\":\n",
    "     # Sum lithk_delta values by subregion column\n",
    "    basin_mass_change_sums = joined_gdf.groupby('SUBREGION1')['lithk_delta'].sum()\n",
    "elif icesheet == \"AIS\":\n",
    "    # Sum lithk_delta values by subregion column\n",
    "    basin_mass_change_sums = joined_gdf.groupby('Subregion')['lithk_delta'].sum()\n",
    "    # Sum lithk_delta values by the 'Regions' column\n",
    "    region_mass_change_sums = joined_gdf.groupby('Regions')['lithk_delta'].sum()\n",
    "else:\n",
    "    raise ValueError(\"Invalid iceshee value. Must be 'GIS' or 'AIS'.\")\n",
    "\n",
    "# Sum all of the basin mass change\n",
    "model_total_mass_balance= basin_mass_change_sums.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBIE data date format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert fractional years to a precise datetime format\n",
    "def fractional_year_to_date(year):\n",
    "    year_int = int(year)  # Extract the integer part (the full year)\n",
    "    fraction = year - year_int  # Extract the fractional part\n",
    "    \n",
    "    # Start at the beginning of the year\n",
    "    start_of_year = pd.Timestamp(f'{year_int}-01-01')\n",
    "    \n",
    "    # Determine if it's a leap year\n",
    "    if pd.Timestamp(f'{year_int}-12-31').is_leap_year:\n",
    "        total_days_in_year = 366\n",
    "    else:\n",
    "        total_days_in_year = 365\n",
    "    \n",
    "    # Convert the fractional part into the corresponding number of days\n",
    "    fractional_days = fraction * total_days_in_year\n",
    "    \n",
    "    # Add the fractional days to the start of the year to get the correct date\n",
    "    return start_of_year + timedelta(days=fractional_days)\n",
    "\n",
    "\n",
    "# Group the data by year\n",
    "def assign_month_order(group):\n",
    "    # Get the month of the first entry for the year\n",
    "    first_month = group['Date'].dt.month.iloc[0]\n",
    "    \n",
    "    # Create a month order starting from the first month and increasing by 1 for each subsequent entry\n",
    "    group['Month_Order'] = range(first_month, first_month + len(group))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract IMBIE mass balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_MassBalance(obs_filename,start_date,end_date):\n",
    "    \n",
    "    # Load the CSV file\n",
    "    mass_balance_data = pd.read_csv(obs_filename)\n",
    "    \n",
    "    # Column names\n",
    "    date_column = 'Year'\n",
    "    \n",
    "    # Ensure the 'Year' column is treated as float to capture the fractional year part\n",
    "    mass_balance_data['Year'] = mass_balance_data['Year'].astype(float)\n",
    "    \n",
    "    # Apply the conversion function to the 'Year' column\n",
    "    mass_balance_data['Date'] = mass_balance_data['Year'].apply(fractional_year_to_date)\n",
    "  \n",
    "    # Sort the data by 'Date' column to ensure it’s in increasing order of both year and fraction\n",
    "    mass_balance_data = mass_balance_data.sort_values(by='Date')\n",
    "      \n",
    "    # Apply the function to each group of data (grouped by the year)\n",
    "    mass_balance_data = mass_balance_data.groupby(mass_balance_data['Date'].dt.year).apply(assign_month_order)\n",
    "    \n",
    "    # Convert 'Year' column to year-month-01 format where month is 'Month_Order'\n",
    "    mass_balance_data['Year'] = mass_balance_data.apply(lambda row: f\"{row['Date'].year}-{str(row['Month_Order']).zfill(2)}-01\", axis=1)\n",
    "    \n",
    "    # Reset the index to flatten the multi-index structure\n",
    "    mass_balance_data = mass_balance_data.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # Check if the column exists in the DataFrame\n",
    "    if mass_balance_column not in mass_balance_data.columns:\n",
    "        raise ValueError(f\"Error: The column '{mass_balance_column}' does not exist in the CSV file.\")\n",
    "\n",
    "    \n",
    "    # Filter the data for the end date\n",
    "    end_data = mass_balance_data[mass_balance_data['Year'] == end_date]    \n",
    "    if end_data.empty:\n",
    "        raise ValueError(f\"Error: No data available for the end date {end_date}.\")\n",
    "    mass_balance_end_value = end_data[mass_balance_column].iloc[-1]  # Last value before or at the end date\n",
    "\n",
    "    \n",
    "    # Filter the data for one date before the start date\n",
    "    data_before_start_date = mass_balance_data[mass_balance_data[date_column] < start_date]\n",
    "    if data_before_start_date.empty:\n",
    "        raise ValueError(f\"Error: No data available before the start date {start_date}.\")\n",
    "    mass_balance_start_value = data_before_start_date[mass_balance_column].iloc[-1]  # Last value before start date\n",
    "    \n",
    "    # Subtract the two values to get the total mass balance change\n",
    "    IMBIE_total_mass_change_sum = mass_balance_end_value - mass_balance_start_value\n",
    "    \n",
    "    return IMBIE_total_mass_change_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mass balance difference of IMBIE and model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMBIE total mass balance\n",
    "IMBIE_total_mass_change_sum=sum_MassBalance(obs_filename,start_date,end_date)\n",
    "\n",
    "# Calculate difference of IMBIE-model  mass change \n",
    "delta_masschange=IMBIE_total_mass_change_sum-model_total_mass_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all required(regional) files are available for Antarctica\n",
    "if icesheet == \"AIS\":\n",
    "    print_regionalresult_check=[]\n",
    "    if os.path.exists(obs_east_filename) and os.path.exists(obs_west_filename) and os.path.exists(obs_peninsula_filename):\n",
    "        #Check\n",
    "        print_regionalresult_check='YES' \n",
    "        \n",
    "        # Calculate total mass for each region\n",
    "        IMBIE_total_mass_change_sum_east = sum_MassBalance(obs_east_filename,start_date,end_date)\n",
    "        IMBIE_total_mass_change_sum_west = sum_MassBalance(obs_west_filename,start_date,end_date)\n",
    "        IMBIE_total_mass_change_sum_peninsula = sum_MassBalance(obs_peninsula_filename,start_date,end_date)\n",
    "\n",
    "        # Calculate difference of IMBIE-model mass change for each region\n",
    "        delta_masschange_east = IMBIE_total_mass_change_sum_east - region_mass_change_sums['East']\n",
    "        delta_masschange_west = IMBIE_total_mass_change_sum_west - region_mass_change_sums['West']\n",
    "        delta_masschange_peninsula = IMBIE_total_mass_change_sum_peninsula - region_mass_change_sums['Peninsula']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display result and write it to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store rows of data for CSV\n",
    "data_rows = []\n",
    "\n",
    "# Add mass change comparison header as the first row with two columns\n",
    "data_rows.append([f\"Mass change comparison ({mass_balance_type})\", f\"{start_date} - {end_date}\"])\n",
    "\n",
    "# Add column headers for basin mass change\n",
    "data_rows.append(['Basin', 'Model mass change (Gt)', 'IMBIE mass change (Gt)', 'Residual (Gt)'])\n",
    "\n",
    "\n",
    "# Apply formatting to two decimal places for the model mass change\n",
    "formatted_mass_change_sums = basin_mass_change_sums.apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "#Placeholders for 'IMBIE mass change' and 'Residual' columns\n",
    "imbie_mass_change = '--'\n",
    "residual_mass_change = '--'\n",
    "\n",
    "print(f\"\\nMass change comparison ({mass_balance_type}): {start_date} - {end_date}\")\n",
    "# Define column headers with fixed width for alignment\n",
    "print(f\"{'Basin':<10} {'Model mass change (Gt)':<25} {'IMBIE mass change (Gt)':<25} {'Residual (Gt)':<25}\")\n",
    "\n",
    "# Loop through and print each basin's subregion mass change output with fixed-width formatting\n",
    "for subregion, model_mass_change in formatted_mass_change_sums.items():\n",
    "    print(f\"{subregion:<10} {model_mass_change:<25} {imbie_mass_change:<25} {residual_mass_change:<25}\")\n",
    "    # Collect each basin's subregion mass change\n",
    "    data_rows.append([subregion, model_mass_change, imbie_mass_change, residual_mass_change])\n",
    "\n",
    "if icesheet == \"AIS\":\n",
    "    if print_regionalresult_check == 'YES':\n",
    "        # Remove 'Regions' and index name\n",
    "        region_mass_change_sums.name = None\n",
    "        region_mass_change_sums.index.name = None\n",
    "        \n",
    "        # Format the Series without displaying the 'dtype'\n",
    "        formatted_region_mass_change = region_mass_change_sums.apply(lambda x: f\"{x:.2f}\")\n",
    "        \n",
    "        # Define regions, totals, and delta changes\n",
    "        regions = [\"East\", \"West\", \"Peninsula\", \"Islands\"]\n",
    "        IMBIE_totals = [IMBIE_total_mass_change_sum_east, IMBIE_total_mass_change_sum_west, \n",
    "                        IMBIE_total_mass_change_sum_peninsula]\n",
    "        delta_changes = [delta_masschange_east, delta_masschange_west, \n",
    "                         delta_masschange_peninsula]\n",
    "        \n",
    "        # Loop through regions and print formatted output\n",
    "        for region, total, delta in zip(regions, IMBIE_totals, delta_changes):\n",
    "            mass_change = formatted_region_mass_change.get(region, \"N/A\")  # Get the mass change for the region\n",
    "            print(f\"{region:<10} {mass_change:<25} {total:<25.2f} {delta:<25.2f}\")\n",
    "            # Collect each region's mass change\n",
    "            data_rows.append([region, mass_change, f\"{total:.2f}\" if total is not None else \"N/A\", \n",
    "                  f\"{delta:.2f}\" if delta is not None else \"N/A\"])\n",
    "\n",
    "# Print total mass balance with formatted columns\n",
    "print(f\"{'Total':<10} {model_total_mass_balance.round(2):<25} {IMBIE_total_mass_change_sum:<25.2f} {delta_masschange:<25.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the mass comaprision output results to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the base name of the nc file (without .nc extension)\n",
    "nc_base_filename = os.path.basename(nc_filename).replace('.nc', '')\n",
    "\n",
    "# Create the CSV filename by combining the output path and the base nc filename with .csv extension\n",
    "csv_filename = os.path.join(output_path, f\"{nc_base_filename}.csv\")\n",
    "\n",
    "\n",
    "# Add the total mass balance row\n",
    "data_rows.append(['Total', f\"{model_total_mass_balance:.2f}\", f\"{IMBIE_total_mass_change_sum:.2f}\", \n",
    "                  f\"{delta_masschange:.2f}\"])\n",
    "\n",
    "# Convert the data rows into a pandas DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "print(f\"Writing data to CSV file: {csv_filename}\")\n",
    "df.to_csv(csv_filename, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
